{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAC Contest\n",
    "This reference design will help you walk through a design flow of DAC SDC 2023. This is a simplified design to help users get started on the FPGA platform and to understand the overall flow. It does not contain any object detection hardware.\n",
    "\n",
    "If you have any questions, please post on the Slack page (link on SDC website sidebar).\n",
    "\n",
    "### Hardware\n",
    "\n",
    "### Software\n",
    "Note:\n",
    "  * You will not submit your `dac_sdc.py` file, so any changes you make to this file will not be considered during evluation.  \n",
    "  * You can use both PS and PL side to do inference.\n",
    "\n",
    "### Object Detection\n",
    "\n",
    "Object detection will be done on images in batches:\n",
    "  * You will provide a Python callback function that will perform object detection on batch of images.  This callback function wile be called many times.\n",
    "  * The callback function should return the locations of all images in the batch.\n",
    "  * Runtime will be recorded during your callback function.\n",
    "  * Images will be loaded from SD card before each batch is run, and this does not count toward your energy usage or runtime.\n",
    "  \n",
    "### Notebook\n",
    "Your notebook should contain 4 code cells:\n",
    "\n",
    "1. Importing all libraries and creating your Team object.\n",
    "1. Downloading the overlay, compile the code, and performany any one-time configuration.\n",
    "1. Python callback function and any other Python helper functions.\n",
    "1. Running object detection\n",
    "1. Cleanup\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installation\n",
    "\n",
    "## 0.1 Packages\n",
    "\n",
    "We recommend creating a seperate conda environment to run the notebook. You can do so with:\n",
    "\n",
    "```bash\n",
    "conda create --name dac python=3.6.9\n",
    "```\n",
    "\n",
    "Install jupyter notebook:\n",
    "\n",
    "```bash\n",
    "pip install notebook\n",
    "```\n",
    "\n",
    "As given by the contest organizers, the following dependencies should already be satisfied:\n",
    "\n",
    "Your team is responsible to make sure the correct packages are installed. For the contest environment, use the configuration below provided by Nvidia:\n",
    "- [JetPack 4.6.1](https://developer.nvidia.com/embedded/jetpack-sdk-461)\n",
    "    - Ubuntu 18.04\n",
    "    - CUDA 10.2\n",
    "    - cuDNN 8.2.1\n",
    "    - gcc 7.5.0\n",
    "    - python 3.6.9\n",
    "    - TensorRT 8.2.1\n",
    "    \n",
    "If you are using an environment, you might need to add the tensorrt library like this (given that you use archiconda):\n",
    "\n",
    "```bash\n",
    "cp -r /usr/lib/python${PYTHON_VERSION}/dist-packages/tensorrt* ~/archiconda3/envs/dac/lib/python${PYTHON_VERSION}/site-packages/\n",
    "```\n",
    "    \n",
    "We additionally require the following modules:\n",
    "- numpy 1.19.5\n",
    "- Pillow 8.4.0\n",
    "- matplotlib 3.3.4\n",
    "- opencv-python 4.7.0.72\n",
    "- pycuda 2020.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment these lines for installation\n",
    "'''\n",
    "!pip install numpy==1.19.5\n",
    "!pip install Pillow==8.4.0\n",
    "!pip install matplotlib==3.3.4\n",
    "!pip install opencv-python==4.7.0.72\n",
    "!pip install pycuda==2020.1\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Creating trt file\n",
    "As we only have access to a Nvidia Jetson Nano 2GB, but the contest will be evaluated on a Jetson Nano 4GB, the trt file should be regenerated to consider the larger RAM capacity when generating the trt file.\n",
    "\n",
    "In case you have not yet generated the binary for the trtexec, please do so with the following commands\n",
    "\n",
    "\n",
    "``` bash\n",
    "cd /usr/src/tensorrt/samples/trtexec\n",
    "make\n",
    "```\n",
    "\n",
    "You can then generate the new trt file with\n",
    "``` bash\n",
    "/usr/src/tensorrt/bin/trtexec --onnx=norm_simple.onnx --saveEngine=norm_simple.engine --shapes=input:1x3x352x640\n",
    "```\n",
    "\n",
    "Uncomment the following lines to generate the TensorRT Binary (trtexec) and convert the file to tensorrt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /usr/src/tensorrt/samples/trtexec && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec --onnx=simple_final.onnx --saveEngine=simple_final.engine --verbose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Create Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../common\"))\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "import dac_sdc\n",
    "import inference\n",
    "from IPython.display import display\n",
    "\n",
    "team_name = 'CapyNet'\n",
    "dac_sdc.BATCH_SIZE = 1\n",
    "team = dac_sdc.Team(team_name)\n",
    "\n",
    "PATH_TO_ENGINE = 'simple_final.engine'\n",
    "TEST_FILE = '../images_test/00001.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init TRT Logger\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\n",
    "\n",
    "# Load the TRT Plugin\n",
    "trt.init_libnvinfer_plugins(None, '')\n",
    "with open(PATH_TO_ENGINE, 'rb') as f:\n",
    "    engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(f.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python Callback Function and Helper Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing the picture through the pipeline\n",
    "In this example, we use contiguous memory arrays for sending and receiving data via DMA.\n",
    "\n",
    "The size of the buffer depends on the size of the input or output data.  The example images are 640x360 (same size as training and test data), and we will use `pynq.allocate` to allocate contiguous memory.\n",
    "\n",
    "### Callback function\n",
    "The callback function:\n",
    "  - Will be called on each batch of images (will be called many times)\n",
    "  - Is prvided with a list of tuples of (image path, RGB image)\n",
    "  - It should return a dictionary with an entry for each image:\n",
    "    - Key: Image name (`img_path.name`)\n",
    "    - Value: Dictionary of item type and bounding box (keys: `type`, `x`, `y`, `width`, `height`)\n",
    "\n",
    "See the code below for an example:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your team directory where you can access your notebook, and any other files you submit, is available as `team.team_dir`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "inputs, outputs, bindings, stream = inference.allocate_buffers(engine)\n",
    "print(\"Allocate Buffer: {} sec\".format(time.time()-start))\n",
    "\n",
    "start = time.time()\n",
    "context = engine.create_execution_context()\n",
    "print(\"Create Exec: {} sec\".format(time.time()-start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from a test file\n",
    "start = time.time()\n",
    "img = cv2.imread(TEST_FILE)\n",
    "print(\"Img Load: {} sec\".format(time.time()-start))\n",
    "\n",
    "#Get scaling factors\n",
    "x_scale_factor = img.shape[2] / 640\n",
    "y_scale_factor = img.shape[1] / 352\n",
    "\n",
    "# Resize the image (this is part of your runtime)\n",
    "start = time.time()\n",
    "img = cv2.resize(img, (640, 352), interpolation=cv2.INTER_LINEAR)\n",
    "print(\"Resize: {} sec\".format(time.time()-start))\n",
    "\n",
    "start = time.time()\n",
    "img = img.transpose(2, 0, 1)\n",
    "print(\"Preprocess: {} sec\".format(time.time()-start))\n",
    "\n",
    "start = time.time()\n",
    "input_buffer = np.ascontiguousarray(img).reshape(-1)\n",
    "np.copyto(inputs[0].host, input_buffer)\n",
    "print(\"Mem: {} sec\".format(time.time()-start))\n",
    "\n",
    "#do Inference\n",
    "start = time.time()\n",
    "output = inference.do_inference_v2(context,\n",
    "                                   bindings=bindings,\n",
    "                                   inputs=inputs,\n",
    "                                   outputs=outputs,\n",
    "                                   stream=stream)\n",
    "print(\"Inference: {} sec\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_callback(rgb_imgs: list):\n",
    "    \"\"\"Callback function for inference.\n",
    "    Args:\n",
    "        rgb_imgs (list): List of tuples contains Image Paths and Images\n",
    "    \"\"\"\n",
    "    object_locations_by_image = {}\n",
    "    \n",
    "    #image format HWC\n",
    "    for (img_path, img) in rgb_imgs:\n",
    "        \n",
    "        object_locations = []\n",
    "        \n",
    "        x_scale_factor = img.shape[1] / 640\n",
    "        y_scale_factor = img.shape[0] / 352\n",
    "        \n",
    "        img = cv2.resize(img, (640, 352), interpolation=cv2.INTER_LINEAR)\n",
    "        img = img.transpose(2, 0, 1)\n",
    "\n",
    "        input_buffer = np.ascontiguousarray(img).reshape(-1)\n",
    "        np.copyto(inputs[0].host, input_buffer)\n",
    "        \n",
    "        output = inference.do_inference_v2(context,\n",
    "                                           bindings=bindings,\n",
    "                                           inputs=inputs,\n",
    "                                           outputs=outputs,\n",
    "                                           stream=stream)\n",
    "        # Suppress predictions with low confidence scores\n",
    "        thr = 0.4\n",
    "        preds = output[0].reshape(100, 5)\n",
    "        conf_mask = preds[:, 4] > thr\n",
    "        \n",
    "        preds = preds[conf_mask]\n",
    "        labels = output[1][conf_mask]\n",
    "        \n",
    "        for pred in range(len(preds)):\n",
    "            x1 = int(preds[pred][0] * x_scale_factor)\n",
    "            y1 = int(preds[pred][1] * y_scale_factor)\n",
    "            x2 = int(preds[pred][2] * x_scale_factor)\n",
    "            y2 = int(preds[pred][3] * y_scale_factor)\n",
    "            object_locations.append({\"type\": int(labels[pred]) + 1, \"x\": x1, \"y\": y1, \"width\": x2 - x1, \"height\": y2 - y1})\n",
    "        # Save to dictionary by image filename\n",
    "        object_locations_by_image[img_path.name] = object_locations\n",
    "\n",
    "    return object_locations_by_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Object Detection\n",
    "\n",
    "Call the following function to run the object detection.  Extra debug output is enabled when `debug` is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team.run(my_callback, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 ../scripts/score.py ../CapyNet/ ../data/dac/train/label/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd6ff51729431282dc7919ea9d5a4be0481382cdf2eb248be1a23fa6e05127fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
